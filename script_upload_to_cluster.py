import pandas as pd
import json
import argparse
import sys
import os
import psycopg2
import psycopg2.extras
from pathlib import Path
from tqdm import tqdm
import io
import uuid
from datetime import datetime, timedelta
import random

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è Excel-–¥–∞—Ç—ã –≤ datetime
EXCEL_EPOCH = pd.Timestamp('1900-01-01')

def excel_date_to_datetime(excel_date):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç Excel serial date –≤ datetime —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ YYYY-MM-DD HH:MM:SS"""
    try:
        if pd.isna(excel_date) or excel_date == '':
            return None
        # Excel —Å—á–∏—Ç–∞–µ—Ç 1900 –≥–æ–¥ –≤–∏—Å–æ–∫–æ—Å–Ω—ã–º (—á—Ç–æ –Ω–µ–≤–µ—Ä–Ω–æ), –ø–æ—ç—Ç–æ–º—É –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º
        if excel_date > 59:
            excel_date -= 1
        dt = EXCEL_EPOCH + pd.Timedelta(days=excel_date - 2)
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except:
        return None

def parse_chain_data(chain_data):
    """–ü–∞—Ä—Å–∏—Ç —Ü–µ–ø–æ—á–∫—É –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    if not chain_data or pd.isna(chain_data):
        return []
    
    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫ (–∏–∑ JSON)
    if isinstance(chain_data, list):
        return chain_data
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
    if isinstance(chain_data, str):
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–æ–±—Ä–∞—Ç—å –∫–∞–∫ JSON-–º–∞—Å—Å–∏–≤
        chain_data = chain_data.strip()
        if (chain_data.startswith('[') and chain_data.endswith(']')) or \
           (chain_data.startswith('"[') and chain_data.endswith(']"')):
            try:
                return json.loads(chain_data)
            except:
                pass
        
        # –£–±–∏—Ä–∞–µ–º –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        chain_data = chain_data.strip('[]')
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        for separator in ['‚Üê', '‚Üê', ',', ';']:  # ‚Üê (U+2190) –∏ ‚Üê (U+2190)
            if separator in chain_data:
                return [x.strip() for x in chain_data.split(separator) if x.strip()]
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç
        return [chain_data] if chain_data else []
    
    # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤
    return [str(chain_data)]

def analyze_source_data(records):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π"""
    analysis = {
        'processes': set(),
        'chain_lengths': [],
        'timestamps': [],
        'probabilities': [],
        'anomaly_scores': [],
        'hosts': []
    }
    
    for record in records:
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å—ã –∏–∑ —Ü–µ–ø–æ—á–µ–∫
        chain_data = record.get('chain_proc_names') or record.get('chain_proc_info')
        if chain_data:
            chain = parse_chain_data(chain_data)
            if chain:
                analysis['processes'].update(chain)
                analysis['chain_lengths'].append(len(chain))
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–∞–π–º—Å—Ç–∞–º–ø—ã
        ts = record.get('last_changed') or record.get('_last_changed')
        if ts:
            try:
                dt = pd.to_datetime(ts)
                analysis['timestamps'].append(dt)
            except:
                pass
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ö–æ—Å—Ç—ã
        host = record.get('host')
        if host:
            analysis['hosts'].append(host)
        
        # –°–æ–±–∏—Ä–∞–µ–º probability (–∏–∑ step)
        step = record.get('step')
        if step is not None:
            try:
                prob = min(float(step) / 100.0, 1.0)
                analysis['probabilities'].append(prob)
            except:
                pass
        
        # –°–æ–±–∏—Ä–∞–µ–º anomaly_score (–∏–∑ time_like_number)
        tln = record.get('time_like_number')
        if tln is not None:
            try:
                score = abs((float(tln) / 1000000.0) % 1.0)
                analysis['anomaly_scores'].append(score)
            except:
                pass
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –≤ list –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    analysis['processes'] = list(analysis['processes']) if analysis['processes'] else ['procA', 'procB', 'procC', 'procD', 'procE']
    analysis['chain_lengths'] = analysis['chain_lengths'] if analysis['chain_lengths'] else [2, 3, 4]
    analysis['hosts'] = list(set(analysis['hosts'])) if analysis['hosts'] else ['host1', 'host2', 'host3']
    
    return analysis

def generate_synthetic_records(analysis, count, existing_records):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø–∏—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    synthetic = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    min_chain_len = min(analysis['chain_lengths']) if analysis['chain_lengths'] else 2
    max_chain_len = max(analysis['chain_lengths']) if analysis['chain_lengths'] else 4
    
    if len(analysis['timestamps']) >= 2:
        min_date = min(analysis['timestamps'])
        max_date = max(analysis['timestamps'])
    else:
        min_date = pd.Timestamp('2024-01-01')
        max_date = pd.Timestamp('2024-12-31')
    
    time_diff_seconds = int((max_date - min_date).total_seconds())
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –∫–∞–∫ —à–∞–±–ª–æ–Ω—ã –¥–ª—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
    template_records = existing_records[-100:] if len(existing_records) >= 10 else existing_records
    
    print(f" –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {count} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(existing_records)} —Ä–µ–∞–ª—å–Ω—ã—Ö...")
    
    for i in range(count):
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–ø–∏—Å—å –∫–∞–∫ —à–∞–±–ª–æ–Ω (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if template_records:
            template = random.choice(template_records)
            base_chain = template.get('sequence', [])
            base_host = template.get('host', 'unknown')
        else:
            base_chain = []
            base_host = 'unknown'
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é —Ü–µ–ø–æ—á–∫—É
        if analysis['processes']:
            chain_len = random.randint(min_chain_len, max(max_chain_len, min_chain_len + 1))
            if base_chain and len(base_chain) > 1:
                # –í–∞—Ä–∏–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ü–µ–ø–æ—á–∫–∏: —Å–ª—É—á–∞–π–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º/—É–¥–∞–ª—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å—ã
                chain = base_chain[:random.randint(1, len(base_chain))]
                available_processes = [p for p in analysis['processes'] if p not in chain]
                if available_processes and random.random() > 0.5:
                    chain.append(random.choice(available_processes))
            else:
                chain = random.sample(analysis['processes'], min(chain_len, len(analysis['processes'])))
        else:
            chain = ['procA', 'procB', 'procC']
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º timestamp –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if time_diff_seconds > 0:
            random_seconds = random.randint(0, time_diff_seconds)
            timestamp = (min_date + timedelta(seconds=random_seconds)).strftime('%Y-%m-%d %H:%M:%S')
        else:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º probability (—Å –Ω–µ–±–æ–ª—å—à–∏–º —Å–ª—É—á–∞–π–Ω—ã–º –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ–º –æ—Ç —à–∞–±–ª–æ–Ω–∞)
        if analysis['probabilities']:
            base_prob = random.choice(analysis['probabilities'])
        else:
            base_prob = random.uniform(0.3, 1.0)
        probability = max(0.0, min(1.0, base_prob + random.uniform(-0.1, 0.1)))
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º anomaly_score
        if analysis['anomaly_scores']:
            base_score = random.choice(analysis['anomaly_scores'])
        else:
            base_score = random.uniform(0.0, 0.5)
        anomaly_score = max(0.0, min(1.0, base_score + random.uniform(-0.05, 0.05)))
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º host
        if analysis['hosts']:
            host = random.choice(analysis['hosts'])
        else:
            host = base_host if base_host != 'unknown' else f"host_{random.randint(1, 10)}"
        
        record = {
            'trace_id': f"synth_{uuid.uuid4()}",
            'timestamp': timestamp,
            'host': host,
            'sequence': chain,
            'probability': round(probability, 4),
            'anomaly_score': round(anomaly_score, 4),
            'sequence_str': ' -> '.join(chain)
        }
        
        synthetic.append(record)
    
    return synthetic

def xlsx_to_json(xlsx_path, json_path=None):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç XLSX –≤ JSON —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Timestamp –∏ NaN"""
    try:
        print(f"üìÑ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {xlsx_path}")
        
        # –ß–∏—Ç–∞–µ–º XLSX
        df = pd.read_excel(
            xlsx_path, 
            na_values=['', 'NaN', 'NULL', 'null', '#N/A'],
            keep_default_na=True
        )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º _last_changed –≤ last_changed
        if '_last_changed' in df.columns:
            print("üîß –ü—Ä–µ–æ–±—Ä–∞–∑—É—é _last_changed –≤ last_changed...")
            df['last_changed'] = df['_last_changed'].apply(excel_date_to_datetime)
            df.drop(columns=['_last_changed'], inplace=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Timestamp ‚Üí —Å—Ç—Ä–æ–∫–∞ ISO (–∏–ª–∏ None)
        for col in df.columns:
            if df[col].dtype == 'datetime64[ns]':
                df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').where(df[col].notna(), None)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ None
        df = df.dropna(how='all')
        df = df.where(pd.notnull(df), None)
        
        # –ó–∞–º–µ–Ω—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ None –¥–ª—è timestamp-like –∫–æ–ª–æ–Ω–æ–∫
        for col in ['last_changed', 'last_event_uuid', 'proc_meta', 'proc_hash']:
            if col in df.columns:
                df[col] = df[col].replace('', None)
        
        records = df.to_dict(orient='records')
        
        if json_path is None:
            json_path = str(Path(xlsx_path).with_suffix('.json'))
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {json_path} ({len(records):,} –∑–∞–ø–∏—Å–µ–π)")
        return json_path, records
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def convert_to_ml_format(records, desired_count=None):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–∞–ø–∏—Å–∏ –≤ ML-—Ñ–æ—Ä–º–∞—Ç —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    processed_records = []
    trace_id_counts = {}  # –°—á–µ—Ç—á–∏–∫ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ proc_id
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
    if desired_count is not None and desired_count < len(records):
        records_to_process = records[:desired_count]
    else:
        records_to_process = records
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
    for idx, record in enumerate(tqdm(records_to_process, desc="–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ ML-—Ñ–æ—Ä–º–∞—Ç", unit="rec")):
        ml_record = {
            'trace_id': None,
            'timestamp': None,
            'host': None,
            'sequence': [],
            'probability': 1.0,
            'anomaly_score': 0.0,
            'sequence_str': ""
        }
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ trace_id
        if record.get('proc_id') is not None:
            base_trace_id = f"proc_{record['proc_id']}"
            count = trace_id_counts.get(base_trace_id, 0)
            if count == 0:
                ml_record['trace_id'] = base_trace_id
            else:
                ml_record['trace_id'] = f"{base_trace_id}_{count}"
            trace_id_counts[base_trace_id] = count + 1
        else:
            ml_record['trace_id'] = str(uuid.uuid4())
        
        # –û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
        # [–í—Å—Ç–∞–≤—å—Ç–µ –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏ —Å—é–¥–∞]
        
        # host
        ml_record['host'] = record.get('host', 'unknown')
        
        # timestamp
        if record.get('last_changed') is not None:
            ml_record['timestamp'] = record['last_changed']
        else:
            ml_record['timestamp'] = f"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d} {random.randint(0,23):02d}:{random.randint(0,59):02d}:{random.randint(0,59):02d}"
        
        # sequence
        chain_data = record.get('chain_proc_names') or record.get('chain_proc_info')
        ml_record['sequence'] = parse_chain_data(chain_data)
        
        if not ml_record['sequence']:
            proc_name = record.get('proc_name', '')
            parent_name = record.get('parent_proc_name', '')
            if proc_name and parent_name:
                ml_record['sequence'] = [parent_name, proc_name]
            elif proc_name:
                ml_record['sequence'] = [proc_name]
        
        # probability
        if record.get('step') is not None:
            try:
                ml_record['probability'] = min(float(record['step']) / 100.0, 1.0)
            except:
                ml_record['probability'] = 0.5 + (idx % 10) * 0.05
        else:
            ml_record['probability'] = 0.3 + (idx % 70) * 0.01
        
        # anomaly_score
        if record.get('time_like_number') is not None:
            try:
                tln = float(record['time_like_number'])
                ml_record['anomaly_score'] = abs((tln / 1000000.0) % 1.0)
            except:
                ml_record['anomaly_score'] = (idx % 100) / 100.0
        else:
            ml_record['anomaly_score'] = (idx % 100) / 100.0
        
        # sequence_str
        ml_record['sequence_str'] = ' -> '.join(ml_record['sequence']) if ml_record['sequence'] else ""
        
        processed_records.append(ml_record)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø–∏—Å–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if desired_count is not None and len(processed_records) < desired_count:
        additional = desired_count - len(processed_records)
        analysis = analyze_source_data(records)
        synthetic = generate_synthetic_records(analysis, additional, processed_records)
        processed_records.extend(synthetic)
    
    return processed_records

def xlsx_to_ml_json(xlsx_path, json_path=None, save_to_file=True, desired_count=None):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç XLSX –≤ JSON –≤ ML-—Ñ–æ—Ä–º–∞—Ç–µ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    try:
        print(f"üìÑ –ß—Ç–µ–Ω–∏–µ XLSX –¥–ª—è ML-–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {xlsx_path}")
        
        # –ß–∏—Ç–∞–µ–º XLSX
        df = pd.read_excel(
            xlsx_path, 
            na_values=['', 'NaN', 'NULL', 'null', '#N/A'],
            keep_default_na=True
        )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º _last_changed –≤ last_changed
        if '_last_changed' in df.columns:
            print("üîß –ü—Ä–µ–æ–±—Ä–∞–∑—É—é _last_changed –≤ last_changed...")
            df['last_changed'] = df['_last_changed'].apply(excel_date_to_datetime)
            df.drop(columns=['_last_changed'], inplace=True)
        
        # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = df.dropna(how='all')
        df = df.where(pd.notnull(df), None)
        
        records = df.to_dict(orient='records')
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ML-—Ñ–æ—Ä–º–∞—Ç —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
        ml_records = convert_to_ml_format(records, desired_count)
        
        if save_to_file:
            if json_path is None:
                json_path = str(Path(xlsx_path).stem + '_ml.json')
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(ml_records, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ ML-JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {json_path} ({len(ml_records):,} –∑–∞–ø–∏—Å–µ–π)")
            return json_path, ml_records
        else:
            return None, ml_records
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ ML-–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def load_ml_to_postgresql(records, db_config, batch_size=5000, truncate=False):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç ML-–¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É ml_process_logs_modify"""
    try:
        print(f"üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL: {db_config['host']}:{db_config['port']}")
        
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–∞–±–ª–∏—Ü–∞
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = 'ml_process_logs_modify'
            );
        """)
        table_exists = cursor.fetchone()[0]
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS ml_process_logs_modify (
            trace_id TEXT PRIMARY KEY,
            timestamp TIMESTAMP,
            host TEXT,
            sequence JSONB,
            probability REAL,
            anomaly_score REAL,
            sequence_str TEXT,
            loaded_at TIMESTAMP DEFAULT NOW()
        );
        """
        
        cursor.execute(create_table_sql)
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_ml_mod_timestamp ON ml_process_logs_modify(timestamp);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_probability ON ml_process_logs_modify(probability);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_anomaly ON ml_process_logs_modify(anomaly_score);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_host ON ml_process_logs_modify(host);
        """)
        
        if truncate:
            cursor.execute("TRUNCATE ml_process_logs_modify CASCADE;")
            print("üóëÔ∏è  –¢–∞–±–ª–∏—Ü–∞ ml_process_logs_modify –æ—á–∏—â–µ–Ω–∞")
        
        conn.commit()
        
        print(f"üì§ –ó–∞–≥—Ä—É–∑–∫–∞ ML-–¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ COPY (batch: {batch_size})...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        buffer = io.StringIO()
        success_count = 0
        
        for record in tqdm(records, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ ML-–∑–∞–ø–∏—Å–µ–π", unit="rec"):
            try:
                values = []
                
                # trace_id
                values.append(str(record.get('trace_id', '')) or '\\N')
                
                # timestamp
                timestamp = record.get('timestamp')
                values.append(timestamp if timestamp else '\\N')
                
                # host
                host = record.get('host', 'unknown')
                values.append(host if host else '\\N')
                
                # sequence (JSONB)
                sequence = record.get('sequence', [])
                values.append(json.dumps(sequence) if sequence else '\\N')
                
                # probability
                prob = record.get('probability', 1.0)
                values.append(str(prob) if prob is not None else '\\N')
                
                # anomaly_score
                score = record.get('anomaly_score', 0.0)
                values.append(str(score) if score is not None else '\\N')
                
                # sequence_str
                seq_str = record.get('sequence_str', '')
                values.append(seq_str if seq_str else '\\N')
                
                line = '\t'.join(values)
                buffer.write(line + '\n')
                success_count += 1
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –∑–∞–ø–∏—Å–∏ {record.get('trace_id')}: {e}")
                continue
        
        buffer.seek(0)
        
        # COPY
        cursor.copy_from(
            buffer,
            'ml_process_logs_modify',
            null='\\N',
            columns=['trace_id', 'timestamp', 'host', 'sequence', 'probability', 'anomaly_score', 'sequence_str']
        )
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {success_count:,} –∏–∑ {len(records):,} ML-–∑–∞–ø–∏—Å–µ–π –≤ ml_process_logs_modify")
        
        if success_count != len(records):
            print(f"‚ö†Ô∏è  {len(records) - success_count} –∑–∞–ø–∏—Å–µ–π –Ω–µ –±—ã–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫")
        
        return success_count
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ML-–¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def verify_data_in_db(db_config, table_name='ml_process_logs_modify', limit=5):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å –≤ –ë–î"""
    try:
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        
        print(f"\nüìä –í —Ç–∞–±–ª–∏—Ü–µ {table_name} –Ω–∞–π–¥–µ–Ω–æ {count:,} –∑–∞–ø–∏—Å–µ–π")
        
        if count > 0:
            print(f"\n–ü–µ—Ä–≤—ã–µ {limit} –∑–∞–ø–∏—Å–µ–π:")
            cursor.execute(f"SELECT trace_id, timestamp, host, probability, anomaly_score, sequence_str FROM {table_name} ORDER BY timestamp DESC LIMIT {limit}")
            rows = cursor.fetchall()
            for row in rows:
                print(f"  trace_id: {row[0]}")
                print(f"  timestamp: {row[1]}")
                print(f"  host: {row[2]}")
                print(f"  probability: {row[3]}")
                print(f"  anomaly_score: {row[4]}")
                print(f"  sequence_str: {row[5]}")
                print("  ---")
        
        cursor.close()
        conn.close()
        return count
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return -1

def main():
    parser = argparse.ArgumentParser(description='–ó–∞–≥—Ä—É–∑—á–∏–∫ ML-–¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL –∫–ª–∞—Å—Ç–µ—Ä —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π')
    parser.add_argument('--file', '-f', required=True, help='–ü—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É')
    parser.add_argument('--db-host', default='10.0.2.12', help='–•–æ—Å—Ç PostgreSQL –º–∞—Å—Ç–µ—Ä–∞')
    parser.add_argument('--db-port', type=int, default=5432, help='–ü–æ—Ä—Ç PostgreSQL')
    parser.add_argument('--db-name', default='postgres', help='–ò–º—è –ë–î')
    parser.add_argument('--db-user', default='dbadmin', help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ë–î')
    parser.add_argument('--db-pass', help='–ü–∞—Ä–æ–ª—å –ë–î (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∑–∞–ø—Ä–æ—Å–∏—Ç)')
    parser.add_argument('--batch-size', type=int, default=5000, help='–†–∞–∑–º–µ—Ä batch')
    parser.add_argument('--truncate', action='store_true', help='–û—á–∏—Å—Ç–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π')
    parser.add_argument('--dry-run', action='store_true', help='–¢–æ–ª—å–∫–æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è, –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î')
    parser.add_argument('--verify', action='store_true', help='–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –ë–î –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏')
    parser.add_argument('--count', '-c', type=int, help='–ñ–µ–ª–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π. –ï—Å–ª–∏ –±–æ–ª—å—à–µ —á–µ–º –≤ XLSX, –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø–∏—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–æ–≤')
    
    args = parser.parse_args()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ML-—Ñ–æ—Ä–º–∞—Ç
    print("\n" + "="*60)
    print("–®–ê–ì 1: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è XLSX –≤ ML-—Ñ–æ—Ä–º–∞—Ç")
    if args.count:
        print(f"   –ñ–µ–ª–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {args.count:,}")
    print("="*60)
    
    _, ml_records = xlsx_to_ml_json(args.file, save_to_file=True, desired_count=args.count)
    
    if args.dry_run:
        print("\n‚úÖ Dry-run –∑–∞–≤–µ—Ä—à–µ–Ω. –î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ë–î.")
        print(f"   –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(ml_records):,}")
        print(f"   –†–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {min(args.count or len(ml_records), len(pd.read_excel(args.file)))}")
        if args.count and args.count > len(pd.read_excel(args.file)):
            print(f"   –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π: {args.count - len(pd.read_excel(args.file))}")
        sys.exit(0)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –ë–î
    print("\n" + "="*60)
    print("–®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ –≤ PostgreSQL")
    print("="*60)
    
    if not args.db_pass:
        args.db_pass = input(f"üîê –í–≤–µ–¥–∏—Ç–µ –ø–∞—Ä–æ–ª—å –¥–ª—è {args.db_user}@{args.db_host}: ")
    
    db_config = {
        'host': args.db_host,
        'port': args.db_port,
        'database': args.db_name,
        'user': args.db_user,
        'password': args.db_pass,
        'connect_timeout': 10
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
    try:
        conn = psycopg2.connect(**db_config)
        conn.close()
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
        sys.exit(1)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    success_count = load_ml_to_postgresql(ml_records, db_config, args.batch_size, args.truncate)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
    if args.verify:
        print("\n" + "="*60)
        print("–®–ê–ì 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î")
        print("="*60)
        verify_data_in_db(db_config, 'ml_process_logs_modify')
    
    print("\n" + "="*60)
    print(f"üéâ –ì–æ—Ç–æ–≤–æ! –î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ ml_process_logs_modify")
    print(f"üìä –ú–∞—Å—Ç–µ—Ä: {args.db_host}")
    print(f"üìà –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {success_count:,} –∑–∞–ø–∏—Å–µ–π")
    if args.count and success_count != args.count:
        print(f"‚ö†Ô∏è  –ó–∞–ø—Ä–æ—à–µ–Ω–æ: {args.count:,} –∑–∞–ø–∏—Å–µ–π")
    print("="*60)

if __name__ == '__main__':

    main()

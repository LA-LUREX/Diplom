#!/usr/bin/env python3
"""
ML Data Loader for PostgreSQL Cluster (FIXED for empty timestamps, chain parsing, and duplicate proc_id)
"""

import pandas as pd
import json
import argparse
import sys
import os
import psycopg2
import psycopg2.extras
import random
from pathlib import Path
from tqdm import tqdm
import io
import uuid
from datetime import datetime
from collections import Counter  # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è Excel-–¥–∞—Ç—ã –≤ datetime
EXCEL_EPOCH = pd.Timestamp('1900-01-01')

def excel_date_to_datetime(excel_date):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç Excel serial date –≤ datetime —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ YYYY-MM-DD HH:MM:SS"""
    try:
        if pd.isna(excel_date) or excel_date == '':
            return None
        # Excel —Å—á–∏—Ç–∞–µ—Ç 1900 –≥–æ–¥ –≤–∏—Å–æ–∫–æ—Å–Ω—ã–º (—á—Ç–æ –Ω–µ–≤–µ—Ä–Ω–æ), –ø–æ—ç—Ç–æ–º—É –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º
        if excel_date > 59:
            excel_date -= 1
        dt = EXCEL_EPOCH + pd.Timedelta(days=excel_date - 2)
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except:
        return None

def parse_chain_data(chain_data):
    """–ü–∞—Ä—Å–∏—Ç —Ü–µ–ø–æ—á–∫—É –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    if not chain_data or pd.isna(chain_data):
        return []
    
    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫ (–∏–∑ JSON)
    if isinstance(chain_data, list):
        return chain_data
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
    if isinstance(chain_data, str):
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–æ–±—Ä–∞—Ç—å –∫–∞–∫ JSON-–º–∞—Å—Å–∏–≤
        chain_data = chain_data.strip()
        if (chain_data.startswith('[') and chain_data.endswith(']')) or \
           (chain_data.startswith('"[') and chain_data.endswith(']"')):
            try:
                return json.loads(chain_data)
            except:
                pass
        
        # –£–±–∏—Ä–∞–µ–º –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        chain_data = chain_data.strip('[]')
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        for separator in ['‚Üê', '‚Üê', ',', ';']:  # ‚Üê (U+2190) –∏ ‚Üê (U+2190)
            if separator in chain_data:
                return [x.strip() for x in chain_data.split(separator) if x.strip()]
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç
        return [chain_data] if chain_data else []
    
    # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤
    return [str(chain_data)]

def check_duplicates(records):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã proc_id –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    proc_ids = []
    for record in records:
        proc_id = record.get('proc_id')
        if proc_id is not None:
            proc_ids.append(proc_id)
    
    duplicates = {pid: count for pid, count in Counter(proc_ids).items() if count > 1}
    
    if duplicates:
        print(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è proc_id:")
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        for pid, count in list(duplicates.items())[:10]:
            print(f"   proc_id={pid}: –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è {count} —Ä–∞–∑")
        if len(duplicates) > 10:
            print(f"   ... –∏ –µ—â–µ {len(duplicates) - 10} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        print(f"   –í—Å–µ–≥–æ {len(proc_ids) - len(set(proc_ids))} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π")
    else:
        print("‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ proc_id –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    return bool(duplicates)

def convert_to_ml_format(records):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–∞–ø–∏—Å–∏ –≤ ML-—Ñ–æ—Ä–º–∞—Ç —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ trace_id"""
    processed_records = []
    used_proc_ids = set()  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ proc_id –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    
    for idx, record in enumerate(tqdm(records, desc="–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ ML-—Ñ–æ—Ä–º–∞—Ç", unit="rec")):
        ml_record = {
            'trace_id': None,
            'timestamp': None,
            'host': 'unknown',
            'sequence': [],
            'probability': 1.0,
            'anomaly_score': 0.0,
            'sequence_str': ""
        }
        
        # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ proc_id ===
        proc_id = record.get('proc_id')
        if proc_id is not None:
            base_trace_id = f"proc_{proc_id}"
            if base_trace_id in used_proc_ids:
                # –î—É–±–ª–∏–∫–∞—Ç –Ω–∞–π–¥–µ–Ω - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º UUID
                ml_record['trace_id'] = str(uuid.uuid4())
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 5 –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å)
                if sum(1 for k in used_proc_ids if k.startswith('proc_')) < 5:
                    print(f"‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç proc_id={proc_id}, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω UUID: {ml_record['trace_id'][:8]}...")
            else:
                ml_record['trace_id'] = base_trace_id
                used_proc_ids.add(base_trace_id)
        else:
            ml_record['trace_id'] = str(uuid.uuid4())
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º host
        ml_record['host'] = record.get('host', 'unknown') or 'unknown'
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º timestamp
        if record.get('last_changed') is not None:
            ml_record['timestamp'] = record['last_changed']
        else:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ –≤—Ä–µ–º—è –≤ 2024 –≥–æ–¥—É –¥–ª—è —Ç–µ—Å—Ç–æ–≤
            ml_record['timestamp'] = f"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d} {random.randint(0,23):02d}:{random.randint(0,59):02d}:{random.randint(0,59):02d}"
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º sequence –∏–∑ chain_proc_names –∏–ª–∏ chain_proc_info
        chain_data = record.get('chain_proc_names') or record.get('chain_proc_info')
        ml_record['sequence'] = parse_chain_data(chain_data)
        
        # –ï—Å–ª–∏ sequence –ø—É—Å—Ç–æ–π, –ø—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –∏–∑ proc_name –∏ parent_proc_name
        if not ml_record['sequence']:
            proc_name = record.get('proc_name', '')
            parent_name = record.get('parent_proc_name', '')
            if proc_name and parent_name:
                ml_record['sequence'] = [parent_name, proc_name]
            elif proc_name:
                ml_record['sequence'] = [proc_name]
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º probability
        if record.get('step') is not None:
            try:
                ml_record['probability'] = min(float(record['step']) / 100.0, 1.0)
            except:
                ml_record['probability'] = 0.5 + (idx % 10) * 0.05  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        else:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0.3 –¥–æ 1.0
            ml_record['probability'] = 0.3 + (idx % 70) * 0.01
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º anomaly_score
        if record.get('time_like_number') is not None:
            try:
                tln = float(record['time_like_number'])
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0,1] —á–µ—Ä–µ–∑ —Å–∏–Ω—É—Å –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                ml_record['anomaly_score'] = abs((tln / 1000000.0) % 1.0)
            except:
                ml_record['anomaly_score'] = (idx % 100) / 100.0  # 0.0 to 0.99
        else:
            ml_record['anomaly_score'] = (idx % 100) / 100.0
        
        # –°–æ–∑–¥–∞–µ–º sequence_str
        ml_record['sequence_str'] = ' -> '.join(ml_record['sequence']) if ml_record['sequence'] else ""
        
        processed_records.append(ml_record)
    
    return processed_records

def xlsx_to_json(xlsx_path, json_path=None):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç XLSX –≤ JSON —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Timestamp –∏ NaN"""
    try:
        print(f"üìÑ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {xlsx_path}")
        
        # –ß–∏—Ç–∞–µ–º XLSX
        df = pd.read_excel(
            xlsx_path, 
            na_values=['', 'NaN', 'NULL', 'null', '#N/A'],
            keep_default_na=True
        )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º _last_changed –≤ last_changed
        if '_last_changed' in df.columns:
            print("üîß –ü—Ä–µ–æ–±—Ä–∞–∑—É—é _last_changed –≤ last_changed...")
            df['last_changed'] = df['_last_changed'].apply(excel_date_to_datetime)
            df.drop(columns=['_last_changed'], inplace=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Timestamp ‚Üí —Å—Ç—Ä–æ–∫–∞ ISO (–∏–ª–∏ None)
        for col in df.columns:
            if df[col].dtype == 'datetime64[ns]':
                df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').where(df[col].notna(), None)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ None
        df = df.dropna(how='all')
        df = df.where(pd.notnull(df), None)
        
        # –ó–∞–º–µ–Ω—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ None –¥–ª—è timestamp-like –∫–æ–ª–æ–Ω–æ–∫
        for col in ['last_changed', 'last_event_uuid', 'proc_meta', 'proc_hash']:
            if col in df.columns:
                df[col] = df[col].replace('', None)
        
        records = df.to_dict(orient='records')
        
        if json_path is None:
            json_path = str(Path(xlsx_path).with_suffix('.json'))
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {json_path} ({len(records):,} –∑–∞–ø–∏—Å–µ–π)")
        return json_path, records
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def xlsx_to_ml_json(xlsx_path, json_path=None, save_to_file=True):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç XLSX –≤ JSON –≤ ML-—Ñ–æ—Ä–º–∞—Ç–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    try:
        print(f"üìÑ –ß—Ç–µ–Ω–∏–µ XLSX –¥–ª—è ML-–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {xlsx_path}")
        
        # –ß–∏—Ç–∞–µ–º XLSX
        df = pd.read_excel(
            xlsx_path, 
            na_values=['', 'NaN', 'NULL', 'null', '#N/A'],
            keep_default_na=True
        )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º _last_changed –≤ last_changed
        if '_last_changed' in df.columns:
            print("üîß –ü—Ä–µ–æ–±—Ä–∞–∑—É—é _last_changed –≤ last_changed...")
            df['last_changed'] = df['_last_changed'].apply(excel_date_to_datetime)
            df.drop(columns=['_last_changed'], inplace=True)
        
        # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = df.dropna(how='all')
        df = df.where(pd.notnull(df), None)
        
        records = df.to_dict(orient='records')
        
        # === –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã ===
        print("\n" + "="*50)
        print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ proc_id...")
        print("="*50)
        has_duplicates = check_duplicates(records)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ML-—Ñ–æ—Ä–º–∞—Ç
        ml_records = convert_to_ml_format(records)
        
        if save_to_file:
            if json_path is None:
                json_path = str(Path(xlsx_path).stem + '_ml.json')
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(ml_records, f, ensure_ascii=False, indent=2)
            
            print(f"\n‚úÖ ML-JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {json_path} ({len(ml_records):,} –∑–∞–ø–∏—Å–µ–π)")
            if has_duplicates:
                print("‚úÖ –î—É–±–ª–∏–∫–∞—Ç—ã –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é UUID")
            return json_path, ml_records
        else:
            return None, ml_records
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ ML-–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def load_ml_to_postgresql(records, db_config, batch_size=5000, truncate=False):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç ML-–¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É ml_process_logs_modify"""
    try:
        print(f"üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL: {db_config['host']}:{db_config['port']}")
        
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–∞–±–ª–∏—Ü–∞
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = 'ml_process_logs_modify'
            );
        """)
        table_exists = cursor.fetchone()[0]
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç (–ù–ï —É–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é!)
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS ml_process_logs_modify (
            trace_id TEXT PRIMARY KEY,
            timestamp TIMESTAMP,
            host TEXT,
            sequence JSONB,
            probability REAL,
            anomaly_score REAL,
            sequence_str TEXT,
            loaded_at TIMESTAMP DEFAULT NOW()
        );
        """
        
        cursor.execute(create_table_sql)
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_ml_mod_timestamp ON ml_process_logs_modify(timestamp);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_probability ON ml_process_logs_modify(probability);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_anomaly ON ml_process_logs_modify(anomaly_score);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_host ON ml_process_logs_modify(host);
        """)
        
        if truncate:
            cursor.execute("TRUNCATE ml_process_logs_modify CASCADE;")
            print("üóëÔ∏è  –¢–∞–±–ª–∏—Ü–∞ ml_process_logs_modify –æ—á–∏—â–µ–Ω–∞")
        
        conn.commit()
        
        print(f"üì§ –ó–∞–≥—Ä—É–∑–∫–∞ ML-–¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ COPY (batch: {batch_size})...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        buffer = io.StringIO()
        success_count = 0
        
        for record in tqdm(records, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ ML-–∑–∞–ø–∏—Å–µ–π", unit="rec"):
            try:
                values = []
                
                # trace_id
                values.append(str(record.get('trace_id', '')) or '\\N')
                
                # timestamp
                timestamp = record.get('timestamp')
                values.append(timestamp if timestamp else '\\N')
                
                # host
                host = record.get('host', 'unknown')
                values.append(host if host else '\\N')
                
                # sequence (JSONB)
                sequence = record.get('sequence', [])
                values.append(json.dumps(sequence) if sequence else '\\N')
                
                # probability
                prob = record.get('probability', 1.0)
                values.append(str(prob) if prob is not None else '\\N')
                
                # anomaly_score
                score = record.get('anomaly_score', 0.0)
                values.append(str(score) if score is not None else '\\N')
                
                # sequence_str
                seq_str = record.get('sequence_str', '')
                values.append(seq_str if seq_str else '\\N')
                
                line = '\t'.join(values)
                buffer.write(line + '\n')
                success_count += 1
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –∑–∞–ø–∏—Å–∏ {record.get('trace_id')}: {e}")
                continue
        
        buffer.seek(0)
        
        # COPY
        cursor.copy_from(
            buffer,
            'ml_process_logs_modify',
            null='\\N',
            columns=['trace_id', 'timestamp', 'host', 'sequence', 'probability', 'anomaly_score', 'sequence_str']
        )
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {success_count:,} –∏–∑ {len(records):,} ML-–∑–∞–ø–∏—Å–µ–π –≤ ml_process_logs_modify")
        
        if success_count != len(records):
            print(f"‚ö†Ô∏è  {len(records) - success_count} –∑–∞–ø–∏—Å–µ–π –Ω–µ –±—ã–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫")
        
        return success_count
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ML-–¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def verify_data_in_db(db_config, table_name='ml_process_logs_modify', limit=5):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å –≤ –ë–î"""
    try:
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        
        print(f"\nüìä –í —Ç–∞–±–ª–∏—Ü–µ {table_name} –Ω–∞–π–¥–µ–Ω–æ {count:,} –∑–∞–ø–∏—Å–µ–π")
        
        if count > 0:
            print(f"\n–ü–µ—Ä–≤—ã–µ {limit} –∑–∞–ø–∏—Å–µ–π:")
            cursor.execute(f"SELECT trace_id, timestamp, host, probability, anomaly_score, sequence_str FROM {table_name} ORDER BY timestamp DESC LIMIT {limit}")
            rows = cursor.fetchall()
            for row in rows:
                print(f"  trace_id: {row[0]}")
                print(f"  timestamp: {row[1]}")
                print(f"  host: {row[2]}")
                print(f"  probability: {row[3]}")
                print(f"  anomaly_score: {row[4]}")
                print(f"  sequence_str: {row[5]}")
                print("  ---")
        
        cursor.close()
        conn.close()
        return count
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return -1

def main():
    parser = argparse.ArgumentParser(description='–ó–∞–≥—Ä—É–∑—á–∏–∫ ML-–¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL –∫–ª–∞—Å—Ç–µ—Ä')
    parser.add_argument('--file', '-f', required=True, help='–ü—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É')
    parser.add_argument('--db-host', default='10.0.2.12', help='–•–æ—Å—Ç PostgreSQL –º–∞—Å—Ç–µ—Ä–∞')
    parser.add_argument('--db-port', type=int, default=5432, help='–ü–æ—Ä—Ç PostgreSQL')
    parser.add_argument('--db-name', default='postgres', help='–ò–º—è –ë–î')
    parser.add_argument('--db-user', default='dbadmin', help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ë–î')
    parser.add_argument('--db-pass', help='–ü–∞—Ä–æ–ª—å –ë–î (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∑–∞–ø—Ä–æ—Å–∏—Ç)')
    parser.add_argument('--batch-size', type=int, default=5000, help='–†–∞–∑–º–µ—Ä batch')
    parser.add_argument('--truncate', action='store_true', help='–û—á–∏—Å—Ç–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π')
    parser.add_argument('--dry-run', action='store_true', help='–¢–æ–ª—å–∫–æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è, –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î')
    parser.add_argument('--verify', action='store_true', help='–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –ë–î –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏')
    
    args = parser.parse_args()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ML-—Ñ–æ—Ä–º–∞—Ç
    print("\n" + "="*60)
    print("–®–ê–ì 1: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è XLSX –≤ ML-—Ñ–æ—Ä–º–∞—Ç")
    print("="*60)
    
    _, ml_records = xlsx_to_ml_json(args.file, save_to_file=True)
    
    if args.dry_run:
        print("\n‚úÖ Dry-run –∑–∞–≤–µ—Ä—à–µ–Ω. –î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ë–î.")
        print(f"   –°–æ–∑–¥–∞–Ω–æ {len(ml_records)} ML-–∑–∞–ø–∏—Å–µ–π")
        sys.exit(0)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –ë–î
    print("\n" + "="*60)
    print("–®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ –≤ PostgreSQL")
    print("="*60)
    
    if not args.db_pass:
        args.db_pass = input(f"üîê –í–≤–µ–¥–∏—Ç–µ –ø–∞—Ä–æ–ª—å –¥–ª—è {args.db_user}@{args.db_host}: ")
    
    db_config = {
        'host': args.db_host,
        'port': args.db_port,
        'database': args.db_name,
        'user': args.db_user,
        'password': args.db_pass,
        'connect_timeout': 10
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
    try:
        conn = psycopg2.connect(**db_config)
        conn.close()
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
        sys.exit(1)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    success_count = load_ml_to_postgresql(ml_records, db_config, args.batch_size, args.truncate)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
    if args.verify:
        print("\n" + "="*60)
        print("–®–ê–ì 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î")
        print("="*60)
        verify_data_in_db(db_config, 'ml_process_logs_modify')
    
    print("\n" + "="*60)
    print(f"üéâ –ì–æ—Ç–æ–≤–æ! –î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ ml_process_logs_modify")
    print(f"üìä –ú–∞—Å—Ç–µ—Ä: {args.db_host}")
    print(f"üìà –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {success_count:,} –∑–∞–ø–∏—Å–µ–π")
    if success_count != len(ml_records):
        print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(ml_records) - success_count}")
    print("="*60)

if __name__ == '__main__':
    main()
(обязательное)
Ansible-конфигурация инвентаря и переменных

А.1 ФАЙЛ ansible.cfg

[defaults]
inventory = ./inventory/
roles_path = ./roles/

А.2 ФАЙЛ hosts.yml

all:
  children:
    opensearch_nodes:
      hosts:
        node1:
          ansible_host: 10.0.2.9
          ansible_user: node1
        node2:
          ansible_host: 10.0.2.17
          ansible_user: node2
        node3:
          ansible_host: 10.0.2.16
          ansible_user: node3
    monitor_node:
      hosts:
        monit:
          ansible_host: 10.0.2.11
          ansible_user: monit 
postgres_master:
      hosts:
        pg_node1: 
Продолжение приложения А
     
     ansible_host: 10.0.2.12
          ansible_user: node1
    postgres_replicas:
      hosts:
        pg_node2:
          ansible_host: 10.0.2.14
          ansible_user: node2
        pg_node3:
          ansible_host: 10.0.2.15
          ansible_user: node3
  vars:
    ansible_become_password: "mnxa2003"
ansible_ssh_private_key_file: ~/.ssh/id_ed25519
    ansible_become: true
    ansible_port: 22
А.3 ФАЙЛ all.yml

postgres_user: dbadmin
postgres_password: mnxa2003
postgres_db: postgres
postgres_port: 5432
replication_user: repluser
replication_password: repl_pass
wal_level: replica
max_wal_senders: 5
max_replication_slots: 5
replication_net: "10.0.2.0/24"
sync_replica_name: "pg_node2"
pg_master_data: /opt/pg_master_data
pg_master_config: /opt/pg_master_config
pg_replica_data: /opt/pg_replica_data 
ПРИЛОЖЕНИЕ Б
(обязательное)
Ansible-роли для развертывания кластера OpenSearch

Б.1 ФАЙЛ openseacrh_cluster.yml

- name: Установка Opensearch нод для кластера (3 ноды)
  hosts: opensearch_nodes
  roles:
- opensearch_node
  tags: openseacrh

Б.2 ФАЙЛ main.yml

- name: Создать директории для конфигурации и данных
  file:
    path: “{{ item }}”
    state: directory
    owner: 1000
    group: 1000
    mode: ‘0700’
  loop:
- /opt/opensearch/config
    - /opt/opensearch/data
- name: Install rights recursion
  command: chown -R 1000:1000 /opt/opensearch/data
  when: ansible_os_family == “Debian”
- name: Скопировать шаблон opensearch.yml
  template:
    src: opensearch.yml.j2
    dest: /opt/opensearch/config/opensearch.yml
- name: Скопировать шаблон Dockerfile для custom image
  template: 
Продолжение приложения Б

    src: Dockerfile.j2
    dest: /opt/opensearch/Dockerfile
- name: Скопировать шаблон docker-compose для ноды
  template:
    src: docker-compose-node.yml.j2
    dest: /opt/opensearch/docker-compose.yml
- name: Сборка custom OpenSearch образа (с экспортёром)
  community.docker.docker_image_build:
    name: opensearch-exporter-node:latest

    path: /opt/opensearch
    dockerfile: Dockerfile
- name: Запустить контейнер ноды OpenSearch
  community.docker.docker_compose_v2:
    project_src: /opt/opensearch
    state: present

Б.3 ФАЙЛ docker-compose-node.yml.j2

services:
  opensearch:
    build: .
    image: opensearch-exporter-node:latest
    container_name: {{ inventory_hostname }}
    environment:
      - OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g
    ulimits:
      memlock:
        soft: -1
        hard: -1
 
Продолжение приложения Б

nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./config/opensearch.yml:/usr/share/opensearch/config/opensearch.yml:ro
      - ./data:/usr/share/opensearch/data
    ports:
      - “9200:9200”
      - “9300:9300”
    networks:
      - os-net
networks:
  os-net:
    driver: bridge

Б.4 ФАЙЛ Dockerfile.j2

FROM opensearchproject/opensearch:2.17.1
USER root
RUN ./bin/opensearch-plugin install –batch \
    https://github.com/Aiven-Open/ rometheus-exporter-plugin-for-opensearch/releases/download/2.17.1.0/ rometheus-exporter-2.17.1.0.zip
RUN yum install iputils -y
USER opensearch

Б.5 ФАЙЛ opensearch.yml.j2

cluster.name: “os-cluster”
node.name: “{{ inventory_hostname }}”
node.roles: [“cluster_manager”,”data”,”ingest”]
 
Продолжение приложения Б

network.host: 0.0.0.0
network.publish_host: {{ ansible_host }}
transport.port: 9300
http.port: 9200
discovery.seed_hosts:
{% for host in groups[‘opensearch_nodes’] %}
  - “{{ hostvars[host].ansible_host }}”
{% endfor %}
cluster.initial_cluster_manager_nodes:
{% for h in groups[‘opensearch_nodes’] %}
 - «{{ h }}»
{% endfor %}
plugins.security.disabled: true
bootstrap.memory_lock: true
path.data: /usr/share/opensearch/data
path.logs: /usr/share/opensearch/logs
 rometheus.metric_name.prefix: “opensearch_”

 
ПРИЛОЖЕНИЕ В
(обязательное)
Ansible-роли для развертывания кластера PostgreSQL

В.1 ФАЙЛ postgresql_cluster.yml

- name: Подготовка и запуск мастера
  hosts: postgres_master
  become: true
  roles:
    - role: postgres_master
- name: Подготовка и запуск реплик
  hosts: postgres_replicas
  become: true
  roles:
    - role: postgres_replicas
- name: Проверка статуса репликации и вывода результата
  hosts: postgres_master
  become: true
  tasks:
    - name: Show pg_stat_replication on master
  
shell: docker exec pg_master psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -c "SELECT pid, application_name, client_addr, state, sync_state, sent_lsn, write_lsn, flush_lsn, replay_lsn FROM pg_stat_replication;"
      register: stats
      changed_when: false
    - debug: var=stats.stdout_lines
- name: Проверка реплик (pg_is_in_recovery)
  hosts: postgres_replicas
  become: true
  tasks:
    - name: Проверка статуса репликации
 
Продолжение приложения В

      shell: docker exec pg_replica_{{ inventory_hostname }} psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -tAc "SELECT pg_is_in_recovery();"
  register: rec
      changed_when: false
    - debug:
        msg: "{{ inventory_hostname }} pg_is_in_recovery = {{ rec.stdout | trim }}"

В.2 ФАЙЛ postgresql_exporter.yml

- name: Установка экспортеров на все ноды кластера PostgreSQL
  hosts: postgres_master:postgres_replicas
  become: true
  roles:
    - role: exporter_pg

В.3 ФАЙЛ postgres_master/main.yml

- name: Ensure data and config directories exist
  file:
    path: "{{ item }}"
    state: directory
    owner: "{{ pg_container_uid }}"
    group: "{{ pg_container_gid }}"
    mode: "0700"
  loop:
    - "{{ pg_master_data }}"   
 - "{{ pg_master_config }}"
- name: Render pg_hba.conf from template
  template:
    src: pg_hba.conf.j2
    dest: "{{ pg_master_config }}/pg_hba.conf" 
Продолжение приложения В

    owner: "{{ pg_container_uid }}"
    group: "{{ pg_container_gid }}"
    mode: "0600"
- name: Write postgresql.conf (basic settings)
  copy:
    dest: "{{ pg_master_config }}/postgresql.conf"
    content: |
      listen_addresses = '*'
      port = {{ postgres_port }}
      wal_level = {{ wal_level }}
      max_wal_senders = {{ max_wal_senders }}
     max_replication_slots = {{ max_replication_slots }}
      wal_keep_size = 1GB
      password_encryption = md5
      ssl = off
      synchronous_standby_names = ''
      hba_file = '/etc/postgresql/pg_hba.conf'
      shared_preload_libraries = 'pg_stat_statements'
      pg_stat_statements.max = 10000
      pg_stat_statements.track = all
    owner: "{{ pg_container_uid }}"
    group: "{{ pg_container_gid }}"
    mode: "0600"
- name: Remove existing container (if any)
  community.docker.docker_container:
    name: "{{ postgres_container_name }}"
    state: absent
  ignore_errors: yes
- name: Remove old data dir (if you want a clean init) - SKIPPABLE
  file:
    path: "{{ pg_master_data }}"
 
Продолжение приложения В

 state: directory
- name: Start Postgres master container
  community.docker.docker_container:
    name: "{{ postgres_container_name }}"
   image: "{{ postgres_image }}"
    env:
      POSTGRES_USER: "{{ postgres_user }}"
      POSTGRES_PASSWORD: "{{ postgres_password }}"
      POSTGRES_DB: "{{ postgres_db }}"
    volumes:
      - "{{ pg_master_data }}:/var/lib/postgresql/data"
      - "{{ pg_master_config }}/postgresql.conf:/etc/postgresql/postgresql.conf:ro"
      - "{{ pg_master_config }}/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro"
    ports:
      - "0.0.0.0:{{ postgres_port }}:5432"
    command: "postgres -c config_file=/etc/postgresql/postgresql.conf"
    restart: yes
    state: started
- name: Wait until Postgres responds (pg_isready)
  shell: |
  for i in $(seq 1 30); do
      docker exec {{ postgres_container_name }} pg_isready -U {{ postgres_user }} -d {{ postgres_db }} >/dev/null 2>&1 && exit 0
      sleep 2
done
    exit 1
  register: pg_ready
  failed_when: pg_ready.rc != 0
  changed_when: false
- name: Ensure synchronous_standby_names is empty
  community.docker.docker_container_exec:
 
Продолжение приложения В

    container: "{{ postgres_container_name }}"
    command: >
      psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -c
      "ALTER SYSTEM SET synchronous_standby_names = '';"
  register: alter_sync
  changed_when: "'ALTER SYSTEM' in (alter_sync.stdout | default(''))"
- name: Reload Postgres config after updating pg_hba
  community.docker.docker_container_exec:
    container: "{{ postgres_container_name }}"
command: "psql -U {{ postgres_user }} -d {{ postgres_db }} -c \"SELECT pg_reload_conf();\""
  register: reload_hba
  failed_when: reload_hba.rc != 0
changed_when: false
- name: Wait a couple seconds after reload
  pause:
    seconds: 2
- name: Check if replication role exists
  community.docker.docker_container_exec:
    container: "{{ postgres_container_name }}"
    command: >
      psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -tAc
      "SELECT 1 FROM pg_roles WHERE rolname='{{ replication_user }}';"
  register: role_check
  changed_when: false
- name: Create replication role with retries (safe, kills conflicting CREATE ROLE)
  block:
    - name: Try to create role (single attempt)
      community.docker.docker_container_exec:
  container: "{{ postgres_container_name }}"
        command: >
 
Продолжение приложения В

          psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -v ON_ERROR_SТОП-10=1 -c
"CREATE ROLE {{ replication_user }} WITH LOGIN REPLICATION PASSWORD '{{ replication_password }}';"
      register: create_role
      failed_when: create_role.rc != 0 and "'already exists' not in (create_role.stderr | lower)"
      changed_when: create_role.rc == 0
  rescue:
    - name: Gather blocking PIDs for pg_authid (if any)
      community.docker.docker_container_exec:
        container: "{{ postgres_container_name }}"
        command: >
          psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -tAc
          "SELECT pid FROM pg_locks l JOIN pg_stat_activity a ON l.pid=a.pid
           WHERE relation::text LIKE 'pg_authid%' AND pid <> pg_backend_pid();"
      register: blocking_pids
      changed_when: false
      failed_when: false
   - name: Kill blocking PIDs
      when: blocking_pids.stdout|trim != ""
      vars:
pid_list: "{{ blocking_pids.stdout.splitlines() | map('trim') | reject('equalto','') | list }}"
      loop: "{{ pid_list }}"
      loop_control:
        loop_var: blocked_pid
      community.docker.docker_container_exec:
        container: "{{ postgres_container_name }}"
        command: >
          psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -c
          "SELECT 
pg_terminate_backend({{ blocked_pid }});
 
Продолжение приложения В

      register: term_result
      changed_when: true
      failed_when: false
    - name: Retry create role after cleaning blockers
community.docker.docker_container_exec:
        container: "{{ postgres_container_name }}"
        command: >
          psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -v ON_ERROR_SТОП-10=1 -c
"CREATE ROLE {{ replication_user }} WITH LOGIN REPLICATION PASSWORD '{{ replication_password }}';"
      register: create_role_retry
      failed_when: create_role_retry.rc != 0 and "'already exists' not in (create_role_retry.stderr | lower)"
      changed_when: create_role_retry.rc == 0
- name: Verify replication role exists (final check)
  community.docker.docker_container_exec:
    container: "{{ postgres_container_name }}"
    command: >
      psql -U "{{ postgres_user }}" -d "{{ postgres_db }}" -tAc
      "SELECT rolname, rolreplication FROM pg_roles WHERE rolname='{{ replication_user }}';"
  register: role_verify
  changed_when: false
- name: Fail if role not created
  fail:
    msg: "Replication role {{ replication_user }} was not created; check logs."
  when: role_verify.stdout | trim == ""
- name: Show success
  debug:
 
Продолжение приложения В

msg: "Replication role {{ replication_user }} OK: {{ role_verify.stdout | trim }}"
- name: Remove conflicting postgresql.conf from data directory
  file:
    path: "{{ pg_master_data }}/postgresql.conf"
    state: absent
  ignore_errors: yes
- name: Setup pg_stat_statements
  block:
    - name: Wait for PostgreSQL to be ready
      shell: |
        for i in {1..30}; do
          docker exec {{ postgres_container_name }} pg_isready -U {{ postgres_user }} && exit 0
          sleep 2
        done
        exit 1
      changed_when: false
    - name: Create pg_stat_statements extension
      community.docker.docker_container_exec:
        container: "{{ postgres_container_name }}"
   command: "psql -U {{ postgres_user }} -d {{ postgres_db }} -c 'CREATE EXTENSION IF NOT EXISTS pg_stat_statements;'"
      register: create_ext
      changed_when: "'CREATE EXTENSION' in create_ext.stdout"
    - name: Verify pg_stat_statements is working
      community.docker.docker_container_exec:
        container: "{{ postgres_container_name }}"
        command: "psql -U {{ postgres_user }} -d {{ postgres_db }} -tAc 'SELECT count(*) FROM pg_stat_statements;'"
      register: check_ext
      changed_when: false
      failed_when: check_ext.rc != 0
 
Продолжение приложения В

    - debug:
        msg: "pg_stat_statements is working. Found {{ check_ext.stdout }} queries"
  rescue:
    - debug:
        msg: "Failed to setup pg_stat_statements: {{ create_ext.stderr | default('unknown') }}"
      failed_when: true

В.4 ФАЙЛ pg_hba.conf.j2

local   all             all                                     trust
host    all             all             127.0.0.1/32            trust
host    all             all             172.17.0.0/16            md5
host    all             all             {{ replication_net | default('10.0.2.0/24') }}             md5
{% for host in groups['postgres_replicas'] %}
host    replication     {{ replication_user }}   {{ hostvars[host].ansible_host }}/32    md5
{% endfor %}
host    replication     all                     {{ replication_net | default('10.0.2.0/24') }}            md5

В.5 ФАЙЛ postgres_replicas/main.yml

- name: Sтоп-10 and remove old replica container (if any)
  community.docker.docker_container:
 name: "pg_replica_{{ inventory_hostname }}"
    state: absent
  ignore_errors: true
- name: Remove old replica data (clean)
  file:
    path: "{{ pg_replica_data }}" 
Продолжение приложения В

    state: absent
- name: Create replica data directory
  file:
    path: "{{ pg_replica_data }}"
    state: directory
    owner: 999
    group: 999
    mode: '0700'
- name: Wait for master reachable (simple ping TCP)
  wait_for:
    host: "{{ groups['postgres_master'] | map('extract', hostvars, ['ansible_host']) | list | first }}"
    port: 5432
    timeout: 60
- name: Create base backup via pg_basebackup
  shell: |
 docker run --rm \
      -v {{ pg_replica_data }}:/var/lib/postgresql/data \
      -e PGPASSWORD={{ replication_password }} \
      postgres:15 \
      pg_basebackup -h {{ hostvars[groups['postgres_master'] | list | first].ansible_host }} \
                   -U {{ replication_user }} -D /var/lib/postgresql/data \
                   -Fp -Xs -P -R --no-slot
  register: basebackup
  changed_when: basebackup.rc == 0
  failed_when: basebackup.rc != 0
- name: Ensure primary_conninfo (set application_name for synchronous replica)
  block:
   - name: Build primary_conninfo string
      set_fact:
 
Продолжение приложения В

primary_conninfo: "host={{ hostvars[groups['postgres_master'] | list | first].ansible_host }} port=5432 user={{ replication_user }} password={{ replication_password }} application_name={{ inventory_hostname }}"
    - name: Append primary_conninfo to postgresql.auto.conf (create if missing)
      lineinfile:
        path: "{{ pg_replica_data }}/postgresql.auto.conf"
line: "primary_conninfo = '{{ primary_conninfo }}'"
        create: yes
        owner: 999
        group: 999
  when: basebackup is succeeded
- name: Sтоп-10 and remove old replica container (if any)
  community.docker.docker_container:
    name: "pg_replica_{{ inventory_hostname }}"
    state: absent
    force_kill: true
  ignore_errors: true
- name: Remove old replica data (clean)
  file:
    path: "{{ pg_replica_data }}"
    state: absent
- name: Create replica data directory with correct permissions
  file:
    path: "{{ pg_replica_data }}"
    state: directory
    owner: 999   
 group: 999
    mode: '0700'
- name: Wait for master reachable (TCP ping)
  wait_for:
    host: "{{ groups['postgres_master'] | map('extract', hostvars, ['ansible_host']) | list | first }}"
    port: 5432 
Продолжение приложения В

    timeout: 60
- name: Create base backup via pg_basebackup
  shell: |
    docker run --rm \
      -v {{ pg_replica_data }}:/var/lib/postgresql/data \
      -e PGPASSWORD={{ replication_password }} \
      postgres:15 \
      pg_basebackup -h {{ hostvars[groups['postgres_master'] | list | first].ansible_host }} \
                   -U {{ replication_user }} -D /var/lib/postgresql/data \
                   -Fp -Xs -P -R --no-slot
  register: basebackup
  changed_when: basebackup.rc == 0
  failed_when: basebackup.rc != 0
- name: (CRITICAL)Remove conflicting postgresql.conf from replica data
  file:
    path: "{{ pg_replica_data }}/postgresql.conf"
    state: absent
- name: Ensure correct permissions after backup
file:
    path: "{{ pg_replica_data }}"
    state: directory
    owner: 999
    group: 999
    mode: '0700'
    recurse: yes
- name: Append primary_conninfo to postgresql.auto.conf
  lineinfile:
    path: "{{ pg_replica_data }}/postgresql.auto.conf"
    line: "primary_conninfo = 'host={{ hostvars[groups['postgres_master'] | list | first].ansible_host }} port=5432 user={{ replication_user }} password={{ replication_password }} application_name={{ inventory_hostname }}'"
    create: yes 
Продолжение приложения В

    owner: 999
group: 999
    mode: "0600"
- name: Create custom postgresql.conf for replica
  copy:
dest: "{{ pg_replica_data }}/replica.conf"
    content: |
      listen_addresses = '*'
      port = 5432
      hot_standby = on
      ssl = off
      shared_preload_libraries = 'pg_stat_statements'
      pg_stat_statements.max = 10000
      pg_stat_statements.track = all
    owner: 999
    group: 999
    mode: "0600"
- name: Start Postgres replica container with explicit config
  community.docker.docker_container:
    name: "pg_replica_{{ inventory_hostname }}"
    image: postgres:15
volumes:
      - "{{ pg_replica_data }}:/var/lib/postgresql/data"
    ports:  
    - "5432:5432"
    command: "postgres -c config_file=/var/lib/postgresql/data/replica.conf"
    restart_policy: always
    state: started
- name: Wait 5s
  pause:
    seconds: 5
 
Продолжение приложения В

- name: Check status container replication
  shell: "docker inspect pg_replica_{{ inventory_hostname }} --format='{{ '{{' }}.State.Status{{ '}}' }}'"
  register: container_status
  changed_when: false
- name: Show containet status
  debug:
    msg: "Status pg_replica_{{ inventory_hostname }}: {{ container_status.stdout }}"
- name: Show container log (if not running)
shell: "docker logs pg_replica_{{ inventory_hostname }} --tail 20"
  register: container_logs
  changed_when: false
  when: container_status.stdout != "running"
- name: Fail if container is not running
  fail:
    msg: "Container is not running! Logs: {{ container_logs.stdout }}"
  when: container_status.stdout != "running"
- name: Wait for replica to be ready and in recovery mode
  shell: |
    for i in {1..30}; do
      if docker exec pg_replica_{{ inventory_hostname }} pg_isready -U {{ postgres_user }} -d {{ postgres_db }}; then
        RECOVERY=$(docker exec pg_replica_{{ inventory_hostname }} psql -U {{ postgres_user }} -d {{ postgres_db }} -tAc "SELECT pg_is_in_recovery();")
        if [ "$(echo $RECOVERY | tr -d '[:space:]')" = "t" ]; then
          echo "replica ready"
          exit 0
        fi   
   fi
sleep 2 
ПРИЛОЖЕНИЕ Г
(обязательное)
Ansible-конфигурация системы мониторинга

Г.1 ФАЙЛ monitoring.yml

- name: Install monitoring
  hosts: monitor_node
  roles:
    - monitoring
  tags: monitoring

Г.2 ФАЙЛ monitoring/main.yml

- name: Создать папки мониторинга заново
  file:
    path: "{{ item }}"
    state: directory
    mode: '0755'
  loop:
    - /opt/monitoring/prometheus
    - /opt/monitoring/grafana
    - /opt/monitoring/config
- name: Скопировать docker-compose мониторинга
  template:
    src: docker-compose-monitor.yml.j2
    dest: /opt/monitoring/docker-compose.yml
- name: Скопировать prometheus.yml
  template:
    src: prometheus.yml.j2
    dest: /opt/monitoring/prometheus/prometheus.yml
- name: Запустить контейнеры мониторинга
Продолжение приложения Г

  community.docker.docker_compose_v2:
    project_src: /opt/monitoring
    state: present

Г.3 ФАЙЛ docker-compose-monitor.yml.j2

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
   - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    networks:
      - monit-net
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    networks:
      - monit-net
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.17.1
 
Продолжение приложения Г

container_name: opensearch-dashboards
    environment:
      OPENSEARCH_HOSTS: '["http://node1:9200","http://node2:9200","http://node3:9200"]'
      DISABLE_SECURITY_DASHBOARDS_PLUGIN: "true"
    ports:
      - "5601:5601"
    networks:
      - monit-net
volumes:
  grafana-data: {}
networks:
  monit-net:
    driver: bridge

Г.4 ФАЙЛ grafana.ini.j2

[server]
http_port = 3000
[security]
admin_user = admin
admin_password = admin
[auth]
disable_login_form = false

Г.5 ФАЙЛ prometheus.yml.j2

global:
  scrape_interval: 15s
 
Продолжение приложения Г

  evaluation_interval: 15s
scrape_configs:
  - job_name: 'opensearch'
    metrics_path: '/_prometheus/metrics'
    static_configs:
      - targets:
{% for host in groups['opensearch_nodes'] %}
        - "{{ hostvars[host].ansible_host }}:9200"
{% endfor %}
  - job_name: 'postgres'
    static_configs:
- targets:
{% for host in groups['postgres_master'] + groups['postgres_replicas'] %}
        - "{{ hostvars[host].ansible_host }}:9187"
{% endfor %}
  - job_name: 'cadvisor'
    scrape_interval: 15s
    static_configs:
      - targets:
{% for host in groups['opensearch_nodes'] + groups['postgres_master'] + groups['postgres_replicas'] + groups['monitor_node'] %}
        - "{{ hostvars[host].ansible_host }}:9080"
{% endfor %}
 
ПРИЛОЖЕНИЕ Д
(обязательное)
Python-скрипты конвертации и загрузки данных

Д.1 ФАЙЛ openseacrh_script_loader.py

import pandas as pd
import json
import argparse
import sys
import os
import uuid
from pathlib import Path
from tqdm import tqdm
from datetime import datetime, timedelta
import random
from opensearchpy import OpenSearch, helpers
EXCEL_EPOCH = pd.Timestamp('1900-01-01')
def excel_date_to_datetime(excel_date):
    try:
        if pd.isna(excel_date) or excel_date == '':
            return None
        if excel_date > 59       
excel_date -= 1
        dt = EXCEL_EPOCH + pd.Timedelta(days=excel_date - 2)
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except:
        return None
def parse_chain_data(chain_data):
    if not chain_data or pd.isna(chain_data):
        return []
    if isinstance(chain_data, list):
        return chain_data 
Продолжение приложения Д

    if isinstance(chain_data, str):
        chain_data = chain_data.strip()
        if (chain_data.startswith('[') and chain_data.endswith(']')) or \
           (chain_data.startswith('"[') and chain_data.endswith('"]')):
            try:
                return json.loads(chain_data)
            except:
                pass
        chain_data = chain_data.strip('[]')
                for separator in ['←', '←', ',', ';']:
  if separator in chain_data:
                return [x.strip() for x in chain_data.split(separator) if x.strip()]
        return [chain_data] if chain_data else []
    return [str(chain_data)]
def analyze_source_data(records):
    analysis = {
        'processes': set(),
        'chain_lengths': [],
        'timestamps': [],
        'probabilities': [],
        'anomaly_scores': [],
        'hosts': []
    }
    for record in records:
        chain_data = record.get('chain_proc_names') or record.get('chain_proc_info')
        if chain_data:
            chain = parse_chain_data(chain_data)
            if chain:
                analysis['processes'].update(chain)
                analysis['chain_lengths'].append(len(chain))
 
Продолжение приложения Д

    ts = record.get('last_changed') or record.get('_last_changed')
        if ts:
            try:
                dt = pd.to_datetime(ts)
                analysis['timestamps'].append(dt)
            except:
                pass
        host = record.get('host')
        if host:
            analysis['hosts'].append(host)
        step = record.get('step')
        if step is not None:
            try:
                prob = min(float(step) / 100.0, 1.0)
                analysis['probabilities'].append(prob)
            except:
                pass
        tln = record.get('time_like_number')
        if tln is not None:
            try:
score = abs((float(tln) / 1000000.0) % 1.0)
                analysis['anomaly_scores'].append(score)
            except:
                pass
    analysis['processes'] = list(analysis['processes']) if analysis['processes'] else ['procA', 'procB', 'procC', 'procD', 'procE']
    analysis['chain_lengths'] = analysis['chain_lengths'] if analysis['chain_lengths'] else [2, 3, 4]
    analysis['hosts'] = list(set(analysis['hosts'])) if analysis['hosts'] else ['host1', 'host2', 'host3']
    return analysis
def generate_synthetic_records(analysis, count, existing_records):
    synthetic = [] 
Продолжение приложения Д

    min_chain_len = min(analysis['chain_lengths']) if analysis['chain_lengths'] else 2
    max_chain_len = max(analysis['chain_lengths']) if analysis['chain_lengths'] else 
    if len(analysis['timestamps']) >= 2:
        min_date = min(analysis['timestamps'])
        max_date = max(analysis['timestamps'])
    else:
        min_date = pd.Timestamp('2024-01-01')
        max_date = pd.Timestamp('2024-12-31')
  time_diff_seconds = int((max_date - min_date).total_seconds())
    template_records = existing_records[-100:] if len(existing_records) >= 10 else existing_records
    print(f"Генерация {count} записей...")
    for i in range(count):
        if template_records:
            template = random.choice(template_records)
            base_chain = template.get('sequence', [])
            base_host = template.get('host', 'unknown')
        else:
            base_chain = []
            base_host = 'unknown'
        if analysis['processes']:
            chain_len = random.randint(min_chain_len, max(max_chain_len, min_chain_len + 1))
            if base_chain and len(base_chain) > 1:
                chain = base_chain[:random.randint(1, len(base_chain))]
                available_processes = [p for p in analysis['processes'] if p not in chain]
                if available_processes and random.random() > 0.5:
                    chain.append(random.choice(available_processes))
 
Продолжение приложения Д

            else:
 chain = random.sample(analysis['processes'], min(chain_len, len(analysis['processes'])))
        else:
            chain = ['procA', 'procB', 'procC']
        if time_diff_seconds > 0:
            random_seconds = random.randint(0, time_diff_seconds)
            timestamp = (min_date + timedelta(seconds=random_seconds)).strftime('%Y-%m-%d %H:%M:%S')
        else:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        if analysis['probabilities']:
            base_prob = random.choice(analysis['probabilities'])
        else:
            base_prob = random.uniform(0.3, 1.0)
        probability = max(0.0, min(1.0, base_prob + random.uniform(-0.1, 0.1)))
        if analysis['anomaly_scores']:
            base_score = random.choice(analysis['anomaly_scores'])
        else:
            base_score = random.uniform(0.0, 0.5)
        anomaly_score = max(0.0, min(1.0, base_score + random.uniform(-0.05, 0.05)))
        if analysis['hosts']:
            host = random.choice(analysis['hosts'])
        else:
            host = base_host if base_host != 'unknown' else f"host_{random.randint(1, 10)}"
        record = {
            'trace_id': f"synth_{uuid.uuid4()}",
            'timestamp': timestamp,
            'host': host,
            'sequence': chain,
            'probability': round(probability, 4),
 
Продолжение приложения Д

            'anomaly_score': round(anomaly_score, 4),
            'sequence_str': ' -> '.join(chain)
        }
        synthetic.append(record)
        return synthetic
def xlsx_to_json(xlsx_path, json_path=None):
    try:
        print(f"Чтение файла: {xlsx_path}")
df = pd.read_excel(
            xlsx_path, 
            na_values=['', 'NaN', 'NULL', 'null', '#N/A'],
            keep_default_na=True)
                if '_last_changed' in df.columns:
            df['last_changed'] = df['_last_changed'].apply(excel_date_to_datetime)
            df.drop(columns=['_last_changed'], inplace=True)
        for col in df.columns:
            if df[col].dtype == 'datetime64[ns]':
                df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').where(df[col].notna(), None)
        df = df.dropna(how='all')
        df = df.where(pd.notnull(df), None)
        for col in ['last_changed', 'last_event_uuid', 'proc_meta', 'proc_hash']:
            if col in df.columns:
                df[col] = df[col].replace('', None)
        records = df.to_dict(orient='records')
        if json_path is None:
            json_path = str(Path(xlsx_path).with_suffix('.json'))
 with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        print(f"JSON сохранён: {json_path} ({len(records):,} записей)")
        return json_path, records 
Продолжение приложения Д

    except Exception as e:
        print(f"Ошибка конвертации: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
def convert_to_ml_format(records, desired_count=None):
    processed_records = []
    trace_id_counts = {}
    if desired_count is not None and desired_count < len(records):
        records_to_process = records[:desired_count]
    else:
        records_to_process = records
    for idx, record in enumerate(tqdm(records_to_process, desc="Конвертация в ML-формат", unit="rec")):
        ml_record = {
            'trace_id': None,
 'timestamp': None,
            'host': None,
            'sequence': [],
            'probability': 1.0,
            'anomaly_score': 0.0,
            'sequence_str': ""
        }
        if record.get('proc_id') is not None:
            base_trace_id = f"proc_{record['proc_id']}"
            count = trace_id_counts.get(base_trace_id, 0)
            if count == 0:
                ml_record['trace_id'] = base_trace_id
            else:
                ml_record['trace_id'] = f"{base_trace_id}_{count}"
            trace_id_counts[base_trace_id] = count + 1
        else: 
Продолжение приложения Д

            ml_record['trace_id'] = str(uuid.uuid4())
        ml_record['host'] = record.get('host', 'unknown')
        if record.get('last_changed') is not None:
            ml_record['timestamp'] = record['last_changed']
    else:
            ml_record['timestamp'] = f"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d} {random.randint(0,23):02d}:{random.randint(0,59):02d}:{random.randint(0,59):02d}"
        chain_data = record.get('chain_proc_names') or record.get('chain_proc_info')
        ml_record['sequence'] = parse_chain_data(chain_data)
        if not ml_record['sequence']:
            proc_name = record.get('proc_name', '')
            parent_name = record.get('parent_proc_name', '')
            if proc_name and parent_name:
                ml_record['sequence'] = [parent_name, proc_name]
            elif proc_name:
                ml_record['sequence'] = [proc_name]
        if record.get('step') is not None:
            try:
                ml_record['probability'] = min(float(record['step']) / 100.0, 1.0)
            except:
                ml_record['probability'] = 0.5 + (idx % 10) * 0.05
        else:
            ml_record['probability'] = 0.3 + (idx % 70) * 0.01
        if record.get('time_like_number') is not None:
            try:
                tln = float(record['time_like_number'])
                ml_record['anomaly_score'] = abs((tln / 1000000.0) % 1.0)
            except:
                ml_record['anomaly_score'] = (idx % 100) / 100.0
 
Продолжение приложения Д

        else:
            ml_record['anomaly_score'] = (idx % 100) / 100.0
        ml_record['sequence_str'] = ' -> '.join(ml_record['sequence']) if ml_record['sequence'] else ""
        processed_records.append(ml_record)
    if desired_count is not None and len(processed_records) < desired_count:
        additional = desired_count - len(processed_records)
        analysis = analyze_source_data(records)
        synthetic = generate_synthetic_records(analysis, additional, processed_records)
        processed_records.extend(synthetic)
    return processed_records
def xlsx_to_ml_json(xlsx_path, json_path=None, save_to_file=True, desired_count=None):
    try:
print(f"Чтение XLSX для ML-конвертации: {xlsx_path}")
        df = pd.read_excel(
            xlsx_path, 
            na_values=['', 'NaN', 'NULL', 'null', '#N/A'],
            keep_default_na=True
        )
        if '_last_changed' in df.columns:
            df['last_changed'] = df['_last_changed'].apply(excel_date_to_datetime)
            df.drop(columns=['_last_changed'], inplace=True)
        df = df.dropna(how='all')
        df = df.where(pd.notnull(df), None)
        records = df.to_dict(orient='records')
        ml_records = convert_to_ml_format(records, desired_count)
        if save_to_file:
            if json_path is None:
                json_path = str(Path(xlsx_path).stem + '_ml.json')
            with open(json_path, 'w', encoding='utf-8') as f:
 
Продолжение приложения Д

                json.dump(ml_records, f, ensure_ascii=False, indent=2)
            print(f"ML-JSON сохранён: {json_path} ({len(ml_records):,} записей)")
            return json_path, ml_records
else:
            return None, ml_records
    except Exception as e:
        print(f"Ошибка ML-конвертации: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
def load_ml_to_opensearch(records, os_client, index_name='ml_process_logs_modify', batch_size=5000):
    try:
        print(f"Загрузка {len(records):,} записей в OpenSearch индекс '{index_name}'...")
        if not os_client.indices.exists(index=index_name):
            print(f"Индекс '{index_name}' не существует. Создайте его заранее.")
            sys.exit(1)
        def generate_actions():
            for record in records:
                doc = {
                    'trace_id': record['trace_id'],
                    'timestamp': record['timestamp'],
                    'host': record['host'],
    'sequence': record['sequence'],
                    'probability': float(record['probability']),
                    'anomaly_score': float(record['anomaly_score']),
                    'sequence_str': record['sequence_str']
                }
                yield {
                    "_index": index_name,
 
Продолжение приложения Д

                    "_id": record['trace_id'],
                    "_source": doc
                }
                success_count = 0
        errors = []
                for ok, result in tqdm(helpers.streaming_bulk(
            os_client,
            generate_actions(),
            chunk_size=batch_size,
            raise_on_error=False,
            raise_on_exception=False
        ), total=len(records), desc="Загрузка в OpenSearch", unit="rec"):
            if ok:
success_count += 1
            else:
                errors.append(result)
        if errors:
            print(f"{len(errors)} записей не были загружены:")
            for error in errors[:5]:
                print(f"  {error}")
            if len(errors) > 5:
                print(f"  ... и еще {len(errors) - 5} ошибок")
        print(f"Успешно загружено {success_count:,} из {len(records):,} записей в OpenSearch")
        return success_count
            except Exception as e:
        print(f"Ошибка загрузки в OpenSearch: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
def verify_data_in_opensearch(os_client, index_name='ml_process_logs_modify', limit=5):
 
Продолжение приложения Д

    try:
        count_resp = os_client.count(index=index_name)
   count = count_resp['count']
        print(f"\nВ индексе '{index_name}' найдено {count:,} документов")
        search_resp = os_client.search(
            index=index_name,
            body={
                "query": {"match_all": {}},
                "sort": [{"timestamp": {"order": "desc"}}],
                "size": limit
            }
        )
        hits = search_resp['hits']['hits']
        if hits:
            print(f"\nПервые {limit} документов:")
            for hit in hits:
                source = hit['_source']
                print(f"  ID: {hit['_id']}")
                print(f"    trace_id: {source['trace_id']}")
                print(f"    timestamp: {source['timestamp']}")
                print(f"    host: {source['host']}")
                print(f"    probability: {source['probability']}")
   	     print(f"    anomaly_score: {source['anomaly_score']}")
                print(f"    sequence: {source['sequence']}")
                print(f"    sequence_str: {source['sequence_str']}")
                print("  ---")
        return count
    except Exception as e:
        print(f"Ошибка при проверке данных в OpenSearch: {e}")
        return -1
def main():
 
Продолжение приложения Д

    print("\n" + "="*60)
    print("ШАГ 1: Конвертация XLSX в ML-формат")
    if args.count:
        print(f"   Желаемое количество записей: {args.count:,}")
    print("="*60)
    _, ml_records = xlsx_to_ml_json(args.file, save_to_file=True, desired_count=args.count)
    if args.dry_run:
        print("\nDry-run завершен. Данные не загружены в OpenSearch.")
        print(f"   Всего создано записей: {len(ml_records):,}")
        print(f"   Реальных записей: {min(args.count or len(ml_records), len(pd.read_excel(args.file)))}")
if args.count and args.count > len(pd.read_excel(args.file)):
            print(f"   Синтетических записей: {args.count - len(pd.read_excel(args.file))}")
        sys.exit(0)
    print("\n" + "="*60)
    print("ШАГ 2: Подключение к OpenSearch кластеру")
    print("="*60)
    hosts = [host.strip() for host in args.os_hosts.split(',')]
    print(f"Хосты: {', '.join(hosts)}")
    os_client = OpenSearch(
        hosts=[{'host': host, 'port': args.os_port} for host in hosts],
        http_auth=(args.os_user, args.os_pass) if args.os_user and args.os_pass else None,
        use_ssl=False,
        verify_certs=False,
        ssl_assert_hostname=False,
        ssl_show_warn=False,
    )
    try:
        info = os_client.info()
        print(f"Подключение к OpenSearch успешно")
 
Продолжение приложения Д

print(f"   Кластер: {info['cluster_name']}")
        print(f"   Версия: {info['version']['number']}")
    except Exception as e:
        print(f"Ошибка подключения к OpenSearch: {e}")
        sys.exit(1)
    if not os_client.indices.exists(index=args.os_index):
        print(f"Индекс '{args.os_index}' не существует. Создайте его заранее:")
        print(f"   curl -X PUT http://{hosts[0]}:{args.os_port}/{args.os_index}")
        sys.exit(1)
    print(f"Индекс '{args.os_index}' существует")
    print("\n" + "="*60)
    print("ШАГ 3: Загрузка в OpenSearch")
    print("="*60)
    success_count = load_ml_to_opensearch(ml_records, os_client, args.os_index, args.batch_size)
    if args.verify:
        print("\n" + "="*60)
        print("ШАГ 4: Проверка данных в OpenSearch")
        print("="*60)
        verify_data_in_opensearch(os_client, args.os_index)
print("\n" + "="*60)
    print(f"Готово. Данные загружены в индекс '{args.os_index}'")
    print(f"Хосты: {', '.join(hosts)}")
    print(f"Успешно загружено: {success_count:,} документов")
    if args.count and success_count != args.count:
        print(f"Запрошено: {args.count:,} записей")
    print("="*60)
if __name__ == '__main__':
    main()
 
Продолжение приложения Д

Д.2 ФАЙЛ postgresql_script_loader.py

import pandas as pd
import json
import argparse
import sys
import os
import psycopg2
import psycopg2.extras
from pathlib import Path
from tqdm import tqdm
import io
import uuid
from datetime import datetime, timedelta
import random
EXCEL_EPOCH = pd.Timestamp('1900-01-01')
def excel_date_to_datetime(excel_date):
    try:
        if pd.isna(excel_date) or excel_date == '':
            return None
        if excel_date > 59:
            excel_date -= 1
        dt = EXCEL_EPOCH + pd.Timedelta(days=excel_date - 2)
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except:
        return None
def parse_chain_data(chain_data):
    if not chain_data or pd.isna(chain_data):
        return []
 if isinstance(chain_data, list):
        return chain_data 
Продолжение приложения Д

    if isinstance(chain_data, str):
        chain_data = chain_data.strip()
        if (chain_data.startswith('[') and chain_data.endswith(']')) or \
           (chain_data.startswith('"[') and chain_data.endswith(']"')):
            try:
                return json.loads(chain_data)
            except:
                pass
        chain_data = chain_data.strip('[]')
        for separator in ['←', '←', ',', ';']:  # ← (U+2190) и ← (U+2190)
            if separator in chain_data:
                return [x.strip() for x in chain_data.split(separator) if x.strip()]
        return [chain_data] if chain_data else []
    return [str(chain_data)]
def analyze_source_data(records):
    analysis = {
        'processes': set(),
        'chain_lengths': [],
  'timestamps': [],
        'probabilities': [],
        'anomaly_scores': [],
        'hosts': []
    }
    for record in records:
        chain_data = record.get('chain_proc_names') or record.get('chain_proc_info')
        if chain_data:
            chain = parse_chain_data(chain_data)
            if chain:
                analysis['processes'].update(chain)
                analysis['chain_lengths'].append(len(chain))
 
Продолжение приложения Д

        ts = record.get('last_changed') or record.get('_last_changed')
        if ts:
            try:
                dt = pd.to_datetime(ts)
                analysis['timestamps'].append(dt)
            except:
                pass       
host = record.get('host')
if host:
            analysis['hosts'].append(host)
                step = record.get('step')
        if step is not None:
            try:
                prob = min(float(step) / 100.0, 1.0)
                analysis['probabilities'].append(prob)
            except:
                pass
        tln = record.get('time_like_number')
        if tln is not None:
            try:
                score = abs((float(tln) / 1000000.0) % 1.0)
                analysis['anomaly_scores'].append(score)
            except:
                pass
    analysis['processes'] = list(analysis['processes']) if analysis['processes'] else ['procA', 'procB', 'procC', 'procD', 'procE']
    analysis['chain_lengths'] = analysis['chain_lengths'] if analysis['chain_lengths'] else [2, 3, 4]
analysis['hosts'] = list(set(analysis['hosts'])) if analysis['hosts'] else ['host1', 'host2', 'host3']
    return analysis
def generate_synthetic_records(analysis, count, existing_records):
    synthetic = [] 
Продолжение приложения Д

    min_chain_len = min(analysis['chain_lengths']) if analysis['chain_lengths'] else 2
    max_chain_len = max(analysis['chain_lengths']) if analysis['chain_lengths'] else 
        if len(analysis['timestamps']) >= 2:
        min_date = min(analysis['timestamps'])
        max_date = max(analysis['timestamps'])
    else:
        min_date = pd.Timestamp('2024-01-01')
        max_date = pd.Timestamp('2024-12-31')
    time_diff_seconds = int((max_date - min_date).total_seconds())
    template_records = existing_records[-100:] if len(existing_records) >= 10 else existing_records
    print(f"Генерация {count} синтетических записей на основе {len(existing_records)} реальных...")
    for i in range(count):
        if template_records:
            template = random.choice(template_records)
base_chain = template.get('sequence', [])
            base_host = template.get('host', 'unknown')
        else:
            base_chain = []
            base_host = 'unknown'
                if analysis['processes']:
            chain_len = random.randint(min_chain_len, max(max_chain_len, min_chain_len + 1))
            if base_chain and len(base_chain) > 1:
                chain = base_chain[:random.randint(1, len(base_chain))]
                available_processes = [p for p in analysis['processes'] if p not in chain]
                if available_processes and random.random() > 0.5:
                    chain.append(random.choice(available_processes))
            else:
 
Продолжение приложения Д

                chain = random.sample(analysis['processes'], min(chain_len, len(analysis['processes'])))
        else:
            chain = ['procA', 'procB', 'procC']
        if time_diff_seconds > 0:
            random_seconds = random.randint(0, time_diff_seconds)
timestamp = (min_date + timedelta(seconds=random_seconds)).strftime('%Y-%m-%d %H:%M:%S')
        else:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        if analysis['probabilities']:
            base_prob = random.choice(analysis['probabilities'])
        else:
            base_prob = random.uniform(0.3, 1.0)
        probability = max(0.0, min(1.0, base_prob + random.uniform(-0.1, 0.1)))
        if analysis['anomaly_scores']:
            base_score = random.choice(analysis['anomaly_scores'])
        else:
            base_score = random.uniform(0.0, 0.5)
        anomaly_score = max(0.0, min(1.0, base_score + random.uniform(-0.05, 0.05)))
        if analysis['hosts']:
            host = random.choice(analysis['hosts'])
        else:
            host = base_host if base_host != 'unknown' else f"host_{random.randint(1, 10)}"
record = {
            'trace_id': f"synth_{uuid.uuid4()}",
            'timestamp': timestamp,
            'host': host,
            'sequence': chain,
            'probability': round(probability, 4),
            'anomaly_score': round(anomaly_score, 4),
 
Продолжение приложения Д

            'sequence_str': ' -> '.join(chain)
        }
        synthetic.append(record)
    return synthetic
def xlsx_to_json(xlsx_path, json_path=None):
    try:
        print(f"Чтение файла: {xlsx_path}")
        df = pd.read_excel(
            xlsx_path, 
            na_values=['', 'NaN', 'NULL', 'null', '#N/A'],
            keep_default_na=True
        )        
        if '_last_changed' in df.columns:
print("Преобразую _last_changed в last_changed...")
            df['last_changed'] = df['_last_changed'].apply(excel_date_to_datetime)
            df.drop(columns=['_last_changed'], inplace=True)
        for col in df.columns:
            if df[col].dtype == 'datetime64[ns]':
                df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S').where(df[col].notna(), None)
        df = df.dropna(how='all')
        df = df.where(pd.notnull(df), None)
        for col in ['last_changed', 'last_event_uuid', 'proc_meta', 'proc_hash']:
            if col in df.columns:
                df[col] = df[col].replace('', None)
        records = df.to_dict(orient='records')
        if json_path is None:
            json_path = str(Path(xlsx_path).with_suffix('.json'))
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        print(f"JSON сохранён: {json_path} ({len(records):,} записей)") 
Продолжение приложения Д

        return json_path, records    
    except Exception as e:
print(f"Ошибка конвертации: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
def convert_to_ml_format(records, desired_count=None):
    processed_records = []
    trace_id_counts = {}
        if desired_count is not None and desired_count < len(records):
        records_to_process = records[:desired_count]
    else:
        records_to_process = records
        for idx, record in enumerate(tqdm(records_to_process, desc="Конвертация в ML-формат", unit="rec")):
        ml_record = {
            'trace_id': None,
            'timestamp': None,
            'host': None,
            'sequence': [],
            'probability': 1.0,
            'anomaly_score': 0.0,
'sequence_str': ""
        }
        if record.get('proc_id') is not None:
            base_trace_id = f"proc_{record['proc_id']}"
            count = trace_id_counts.get(base_trace_id, 0)
            if count == 0:
                ml_record['trace_id'] = base_trace_id
            else:
                ml_record['trace_id'] = f"{base_trace_id}_{count}"
            trace_id_counts[base_trace_id] = count + 1 
Продолжение приложения Д

        else:
            ml_record['trace_id'] = str(uuid.uuid4())
        ml_record['host'] = record.get('host', 'unknown')
        if record.get('last_changed') is not None:
            ml_record['timestamp'] = record['last_changed']
        else:
            ml_record['timestamp'] = f"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d} {random.randint(0,23):02d}:{random.randint(0,59):02d}:{random.randint(0,59):02d}"
chain_data = record.get('chain_proc_names') or record.get('chain_proc_info')
        ml_record['sequence'] = parse_chain_data(chain_data)
        if not ml_record['sequence']:
            proc_name = record.get('proc_name', '')
            parent_name = record.get('parent_proc_name', '')
            if proc_name and parent_name:
                ml_record['sequence'] = [parent_name, proc_name]
            elif proc_name:
                ml_record['sequence'] = [proc_name]
        if record.get('step') is not None:
            try:
                ml_record['probability'] = min(float(record['step']) / 100.0, 1.0)
            except:
                ml_record['probability'] = 0.5 + (idx % 10) * 0.05
        else:
            ml_record['probability'] = 0.3 + (idx % 70) * 0.01
        if record.get('time_like_number') is not None:
            try:
                tln = float(record['time_like_number'])
                ml_record['anomaly_score'] = abs((tln / 1000000.0) % 1.0
except:
                ml_record['anomaly_score'] = (idx % 100) / 100.0
 
Продолжение приложения Д

        else:
            ml_record['anomaly_score'] = (idx % 100) / 100.0
        ml_record['sequence_str'] = ' -> '.join(ml_record['sequence']) if ml_record['sequence'] else ""
        processed_records.append(ml_record)
    if desired_count is not None and len(processed_records) < desired_count:
        additional = desired_count - len(processed_records)
        analysis = analyze_source_data(records)
        synthetic = generate_synthetic_records(analysis, additional, processed_records)
        processed_records.extend(synthetic)
    return processed_records
def xlsx_to_ml_json(xlsx_path, json_path=None, save_to_file=True, desired_count=None):
    try:
        print(f"Чтение XLSX для ML-конвертации: {xlsx_path}")
        df = pd.read_excel(
            xlsx_path, 
            na_values=['', 'NaN', 'NULL', 'null', '#N/A'],
  keep_default_na=True
        if '_last_changed' in df.columns:
            print("Преобразую _last_changed в last_changed...")
            df['last_changed'] = df['_last_changed'].apply(excel_date_to_datetime)
            df.drop(columns=['_last_changed'], inplace=True)
        df = df.dropna(how='all')
        df = df.where(pd.notnull(df), None)
        records = df.to_dict(orient='records')
        ml_records = convert_to_ml_format(records, desired_count)
        if save_to_file:
            if json_path is None:
                json_path = str(Path(xlsx_path).stem + '_ml.json')
            with open(json_path, 'w', encoding='utf-8') as f:
 
Продолжение приложения Д

                json.dump(ml_records, f, ensure_ascii=False, indent=2)
            print(f"ML-JSON сохранён: {json_path} ({len(ml_records):,} записей)")
            return json_path, ml_records
        else:
            return None, ml_records
        except Exception as e:
        print(f"Ошибка ML-конвертации: {type(e).__name__}: {e}")
       import traceback
        traceback.print_exc()
        sys.exit(1)
def load_ml_to_postgresql(records, db_config, batch_size=5000, truncate=False):
    try:
        print(f"Подключение к PostgreSQL: {db_config['host']}:{db_config['port']}")
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = 'ml_process_logs_modify'
            );
        """)
        table_exists = cursor.fetchone()[0]
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS ml_process_logs_modify (
            trace_id TEXT PRIMARY KEY,
timestamp TIMESTAMP,
            host TEXT,
            sequence JSONB,
            probability REAL, 
Продолжение приложения Д

            anomaly_score REAL,
            sequence_str TEXT,
            loaded_at TIMESTAMP DEFAULT NOW()
        );
        """
        cursor.execute(create_table_sql)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_ml_mod_timestamp ON ml_process_logs_modify(timestamp);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_probability ON ml_process_logs_modify(probability);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_anomaly ON ml_process_logs_modify(anomaly_score);
            CREATE INDEX IF NOT EXISTS idx_ml_mod_host ON ml_process_logs_modify(host);
        """)
if truncate:
            cursor.execute("TRUNCATE ml_process_logs_modify CASCADE;")
            print("Таблица ml_process_logs_modify очищена")
        conn.commit()
        print(f"Загрузка ML-данных через COPY (batch: {batch_size})...")
        buffer = io.StringIO()
        success_count = 0
        for record in tqdm(records, desc="Обработка ML-записей", unit="rec"):
            try:
                values = []
                values.append(str(record.get('trace_id', '')) or '\\N')
                timestamp = record.get('timestamp')
                values.append(timestamp if timestamp else '\\N')
                host = record.get('host', 'unknown')
                values.append(host if host else '\\N')
                sequence = record.get('sequence', [])
                values.append(json.dumps(sequence) if sequence else '\\N')
 
Продолжение приложения Д

                prob = record.get('probability', 1.0)
                values.append(str(prob) if prob is not None else '\\N')
                score = record.get('anomaly_score', 0.0)
  values.append(str(score) if score is not None else '\\N')
                seq_str = record.get('sequence_str', '')
                values.append(seq_str if seq_str else '\\N')
                line = '\t'.join(values)
                buffer.write(line + '\n')
                success_count += 1
            except Exception as e:
                print(f"Ошибка в записи {record.get('trace_id')}: {e}")
                continue
        buffer.seek(0)
        cursor.copy_from(
            buffer,
            'ml_process_logs_modify',
            null='\\N',
            columns=['trace_id', 'timestamp', 'host', 'sequence', 'probability', 'anomaly_score', 'sequence_str']
        )
        conn.commit()
        cursor.close()
        conn.close()
        print(f"Успешно загружено {success_count:,} из {len(records):,} ML-записей в ml_process_logs_modify")
        if success_count != len(records):
            print(f"{len(records) - success_count} записей не были загружены из-за ошибок")
        return success_count
    except Exception as e:
        print(f"Ошибка загрузки ML-данных: {e}")
        import traceback
        traceback.print_exc() 
Продолжение приложения Д

        sys.exit(1)
def verify_data_in_db(db_config, table_name='ml_process_logs_modify', limit=5):
    try:
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        print(f"\nВ таблице {table_name} найдено {count:,} записей")
                if count > 0:
            print(f"\nПервые {limit} записей:")
cursor.execute(f"SELECT trace_id, timestamp, host, probability, anomaly_score, sequence_str FROM {table_name} ORDER BY timestamp DESC LIMIT {limit}")
            rows = cursor.fetchall()
            for row in rows:
                print(f"  trace_id: {row[0]}")
                print(f"  timestamp: {row[1]}")
                print(f"  host: {row[2]}")
                print(f"  probability: {row[3]}")
                print(f"  anomaly_score: {row[4]}")
                print(f"  sequence_str: {row[5]}")
                print("  ---")
        cursor.close()
        conn.close()
        return count
    except Exception as e:
        print(f"Ошибка при проверке данных: {e}")
        return -1
def main():
    print("\n" + "="*60)
    print("ШАГ 1: Конвертация XLSX в ML-формат")    
if args.count:
 
Продолжение приложения Д

        print(f"   Желаемое количество записей: {args.count:,}")
    print("="*60)
    _, ml_records = xlsx_to_ml_json(args.file, save_to_file=True, desired_count=args.count)
    if args.dry_run:
        print("\nDry-run завершен. Данные не загружены в БД.")
        print(f"   Всего создано записей: {len(ml_records):,}")
        print(f"   Реальных записей: {min(args.count or len(ml_records), len(pd.read_excel(args.file)))}")
        if args.count and args.count > len(pd.read_excel(args.file)):
            print(f"   Синтетических записей: {args.count - len(pd.read_excel(args.file))}")
        sys.exit(0)
        print("\n" + "="*60)
    print("ШАГ 2: Загрузка в PostgreSQL")
    print("="*60)
        if not args.db_pass:
        args.db_pass = input(f"Введите пароль для {args.db_user}@{args.db_host}: ")    
    db_config = {
'host': args.db_host,
        'port': args.db_port,
        'database': args.db_name,
        'user': args.db_user,
        'password': args.db_pass,
        'connect_timeout': 10
    }
    try:
        conn = psycopg2.connect(**db_config)
        conn.close()
        print("Подключение к PostgreSQL успешно")
    except Exception as e:
        print(f"Ошибка подключения к PostgreSQL: {e}")
 
Продолжение приложения Д

        sys.exit(1)
        success_count = load_ml_to_postgresql(ml_records, db_config, args.batch_size, args.truncate)
    if args.verify:
        print("\n" + "="*60)
        print("ШАГ 3: Проверка данных в БД")
        print("="*60)
verify_data_in_db(db_config, 'ml_process_logs_modify')
    print("\n" + "="*60)
    print(f"Готово. Данные доступны в таблице ml_process_logs_modify")
    print(f"Мастер: {args.db_host}")
    print(f"Успешно загружено: {success_count:,} записей")
    if args.count and success_count != args.count:
        print(f" Запрошено: {args.count:,} записей")
    print("="*60)

if __name__ == '__main__':
    main()
